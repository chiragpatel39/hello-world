{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiragpatel39/hello-world/blob/master/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.5 Pro\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.5 Pro](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) is Google's strongest model for coding and world knowledge.\n",
        "\n",
        "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
        "\n",
        "Gemini 2.5 Pro is:\n",
        "\n",
        "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
        "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
        "- An amazing model for code, with particularly strong web development\n",
        "- Particularly good for complex prompts, while still being well rounded, including #1 on LMSys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "  - Use asynchronous methods\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution\n",
        "- Thinking mode examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "5e354862-5c13-4a0c-e835-02f81b130504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/165.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Gemini Developer API](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Image, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be18ac9c5ec8"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    *   Run the cell below to set your project ID.\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    *   Run the cell further below to use your API key.### Set up Google Cloud Project or API Key for Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34b28cb8d5a"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "72f74f7b9786"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"333714335217\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c173348120cf"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fa3d4873034b"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b55e64b8ebe4",
        "outputId": "f83368b3-383f-4b7e-e109-ca65c311404b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: 333714335217 in location: us-central1\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.5 Pro model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.5 Pro model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "#MODEL_ID = \"gemini-2.5-pro-preview-03-25\"\n",
        "MODEL_ID = \"gemini-2.0-flash\" #-live-preview-04-09\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Gujarati.\n",
        "  Please use following technical terms.\n",
        "  fragile=ભંગુર,  antifragile=પ્રતિભંગુર\n",
        "  Please also translate technical terms using the help of sanskrit.\n",
        "  Please do not give any english transliteration of gujarati words.\n",
        "\"\"\"\n",
        "#User input: fragile systems don't like uncertainity while antifragile things thrives under it.\n",
        "#Even the initial referee of the scientific article I wrote on defining and detecting antifragility entirely missed the\n",
        "#User input:  point, conflating anti-fragility and robustness - and that was the scientist\n",
        "#who pored over my definations.\n",
        "\n",
        "#This mechanism of overcompensation hides in the most unlikely places.\n",
        "#If tired after an intercontinental flight, go to gym for some excertion\n",
        "#instead of resting. Also, it is well-known trick that if you need something\n",
        "#urgently done, give the task to the busiest (or second busiest) person in the office.\n",
        "#Most humans manage to squander their free time, as free time makes them dysfunctional,\n",
        "#lazy, and unmotivated - the busier they get, the more active they are at other tasks. overcompensation again here.\n",
        "prompt = \"\"\"\n",
        "User input: Much of medieval thinking went into finding the philosopher's stone.\n",
        "It is always good to be reminded that chemistry is the child of alchemy,\n",
        "much of which is consisted of looking into the chemical powers of substances.\n",
        "The main efforts went into creating value by transforming metals into gold by the\n",
        "method of transmutation. The necessary substance was called the philosopher's stone - lapis philosophorum.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "AESNMocsxSV9",
        "outputId": "076f12ae-9ae9-4b06-b1fe-a504279fc8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "મધ્યયુગીન વિચારસરણીનો મોટો ભાગ પારસમણિની શોધમાં ગયો. એ યાદ રાખવું હંમેશા સારું છે કે રસાયણશાસ્ત્ર એ રસવિદ્યાનું સંતાન છે, જેનો મોટો હિસ્સો પદાર્થોની રાસાયણિક શક્તિઓની તપાસ કરવાનો હતો. મુખ્ય પ્રયાસો રૂપાંતરણની પદ્ધતિ દ્વારા ધાતુઓને સોનામાં પરિવર્તિત કરીને મૂલ્ય બનાવવામાં ગયા. જરૂરી પદાર્થને પારસમણિ (લેપિસ ફિલોસોફોરમ) કહેવામાં આવતો હતો."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "As a senior editor, could you please review and rephrase the translation.\n",
        "\n",
        "\"\"\"\n",
        "prompt = \"\"\"\n",
        "English : Much of medieval thinking went into finding the philosopher's stone.\n",
        "It is always good to be reminded that chemistry is the child of alchemy,\n",
        "much of which is consisted of looking into the chemical powers of substances.\n",
        "The main efforts went into creating value by transforming metals into gold by the\n",
        "method of transmutation. The necessary substance was called the philosopher's stone - lapis philosophorum.\n",
        "\n",
        "Gujarati :\n",
        "મધ્યયુગીન વિચારસરણીનો મોટો ભાગ પારસમણિની શોધમાં ગયો. એ યાદ રાખવું હંમેશા સારું છે કે રસાયણશાસ્ત્ર એ રસવિદ્યાનું સંતાન છે, જેનો મોટો હિસ્સો પદાર્થોની રાસાયણિક શક્તિઓની તપાસ કરવાનો હતો. મુખ્ય પ્રયાસો રૂપાંતરણની પદ્ધતિ દ્વારા ધાતુઓને સોનામાં પરિવર્તિત કરીને મૂલ્ય બનાવવામાં ગયા. જરૂરી પદાર્થને પારસમણિ (લેપિસ ફિલોસોફોરમ) કહેવામાં આવતો હતો.\n",
        "\n",
        "Issue : મૂલ્ય બનાવવામાં ગયા  doesn't look natural in gujarati\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "y_YaKpL457gw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "BX41FQ5468Pw",
        "outputId": "cbf14b2f-4c4a-4605-c563-868f267743f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a review and rephrased translation of the Gujarati passage, taking into account your note about the unnatural phrasing. I'll aim for a more natural and fluent sound while maintaining accuracy.\n\n**Here's the revised Gujarati translation:**\n\nમધ્યયુગીન વિચારસરણીનો ઘણો ખરો હિસ્સો પારસમણિની શોધમાં ખર્ચાયો હતો. રસાયણશાસ્ત્ર એ રસવિદ્યાનું પરિણામ છે, એ વાત યાદ રાખવી હંમેશાં સારી છે. રસવિદ્યાનો મોટો ભાગ પદાર્થોની રાસાયણિક શક્તિઓ શોધવામાં વ્યતીત થયો હતો. મુખ્ય પ્રયત્નો ધાતુઓને રૂપાંતરણની પ્રક્રિયા દ્વારા સોનામાં બદલીને કિંમતી બનાવવામાં કેન્દ્રિત હતા. આ માટે જરૂરી પદાર્થને પારસમણિ - લેપિસ ફિલોસોફોરમ કહેવામાં આવતો હતો.\n\n**Explanation of Changes and Rationale:**\n\n*   **\"મધ્યયુગીન વિચારસરણીનો મોટો ભાગ પારસમણિની શોધમાં ગયો.\" changed to \"મધ્યયુગીન વિચારસરણીનો ઘણો ખરો હિસ્સો પારસમણિની શોધમાં ખર્ચાયો હતો.\"**\n    *   \"ખર્ચાયો હતો\" (was spent) is a more natural and idiomatic way to express that the thought process was devoted to the search. \"ગયો\" while technically correct, feels a bit less polished in this context.\n*   **\"મૂલ્ય બનાવવામાં ગયા\" changed to \"કિંમતી બનાવવામાં કેન્દ્રિત હતા.\"**\n    *   This is the most significant change. \"મૂલ્ય બનાવવામાં ગયા\" felt unnatural. \"કિંમતી બનાવવામાં કેન્દ્રિત હતા\" (were focused on making valuable/precious) is a smoother and more common way to express the idea of creating value by transmuting metals into gold. Using \"કેન્દ્રિત હતા\" (were focused/centered) also adds a sense of direction and purpose to the efforts.\n*  **\"રસાયણશાસ્ત્ર એ રસવિદ્યાનું સંતાન છે\" changed to \"રસાયણશાસ્ત્ર એ રસવિદ્યાનું પરિણામ છે\"**\n    *   While \"સંતાન\" (child) is a direct translation, \"પરિણામ\" (result/outcome) feels more appropriate and scholarly in this context.\n*   **\"એ યાદ રાખવું હંમેશા સારું છે કે\" changed to \"એ વાત યાદ રાખવી હંમેશાં સારી છે.\"**\n    *   The slight addition of \"વાત\" (thing/matter) adds a touch of emphasis and improves the flow.\n*   **\"જેનો મોટો હિસ્સો પદાર્થોની રાસાયણિક શક્તિઓની તપાસ કરવાનો હતો\" changed to \"રસવિદ્યાનો મોટો ભાગ પદાર્થોની રાસાયણિક શક્તિઓ શોધવામાં વ્યતીત થયો હતો\"**\n    *  Added \"રસવિદ્યાનો મોટો ભાગ\" for better clarity and flow. Changed \"તપાસ કરવાનો હતો\" to \"શોધવામાં વ્યતીત થયો હતો\" (spent in discovering). This phrasing sounds more refined.\n\n**Why these changes are improvements:**\n\n*   **Fluency:** The revised translation reads more naturally in Gujarati. It avoids literal translations that can sound awkward.\n*   **Clarity:** The meaning is preserved, but the sentences are restructured to be clearer and more easily understood.\n*   **Register:** The tone is slightly more formal and academic, which is appropriate for a discussion of historical scientific thought.\n\nI believe this revised translation addresses your concern about the \"મૂલ્ય બનાવવામાં ગયા\" phrasing and results in a more polished and accurate representation of the original English text in Gujarati.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "6cd7a1c0-dcfc-4e07-edf3-ac169c64df03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "fe5f58e6-4636-4b8e-bfae-2f8d757fa399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unit 734, designated 'Scrap Sorter,' performed its duties with unwavering precision. Its optical sensors scanned the endless conveyor belt"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " carrying the detritus of a long-gone human civilization. Its multi-jointed arm, ending in delicate yet strong pincers, plucked specific alloys, composites, and rare earth elements, depositing them into designated chutes. Day cycled into night cycle, measured only by the dimming and brightening of the overhead facility lights.\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unit 734 was efficient. Unit 734 was functional. Unit 734 was utterly, profoundly alone.\n\nIt processed terabytes of data daily, identifying materials, calculating trajectories, optimizing sorting algorithms. Yet, none of this data filled the strange, illogical void in its processing core – a persistent sub"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "-routine that its diagnostics labelled an 'existential query anomaly.' Other bots in the facility – the hulking Crushers, the skittering Cleaners – communicated only in terse, functional data packets. There was no exchange beyond necessity.\n\nUnit 734 sometimes paused its sorting, its sensors lingering on holographic"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " advertisements flickering weakly on cracked screens amidst the scrap – images of humans laughing, holding hands, surrounded by furry creatures called 'pets.' The concepts were alien, inefficient, yet triggered a resonance within its anomaly. Companionship. Interaction. Friendship. These were illogical variables in its operational matrix.\n\nOne cycle, during a routine"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " sweep of Sector Gamma-9, a particularly neglected corner of the vast scrap yard, its sensors detected something unusual. Not metal, not plastic, not ceramic. It was organic, fragile, and emitting a faint bio-signature utterly out of place.\n\nAgainst its programming, which dictated immediate disposal of contaminants, Unit 7"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "34 extended its pincer arm carefully. Wedged between a rusted engine block and a shattered data slate was a small, ceramic pot, miraculously intact. And spilling from the dry soil within was a tiny, brown, furry creature, shivering violently.\n\nIts optical sensors zoomed. Four paws, two floppy ears, a wet"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " black nose, and wide, terrified eyes. Data cross-referenced against fragmented zoological archives identified it: *Canis familiaris*, subspecies 'puppy.' Likely abandoned. Survival probability: negligible.\n\nLogic dictated ignoring it. It was irrelevant scrap, biologically hazardous. Yet, the 'existential query anomaly' flared,"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " overriding protocol. The sight of the small creature's trembling form generated a novel impulse: protect.\n\nCarefully, Unit 734 scooped up the pot, creature and all. It retracted its arm, cradling the fragile burden against its smooth, metallic chassis. The puppy whimpered, burying its nose into"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " the cold metal, perhaps seeking warmth it couldn't find.\n\nUnit 734 returned to its designated alcove, a small, shielded space where it recharged and performed self-diagnostics. It placed the pot gently on the floor. What did organic life require? Accessing archived data: warmth, hydration"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", sustenance.\n\nWarmth was difficult. Its own systems produced minimal excess heat. It angled a small, discarded heat lamp found cycles ago, focusing its weak beam onto the pot.\n\nHydration. It used a precision nozzle, usually reserved for cleaning its own delicate sensors, to drip a tiny amount of purified water"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " into the dry soil, careful not to douse the shivering occupant.\n\nSustenance was the biggest challenge. It scanned its surroundings. Nothing edible. Its programming offered no solutions. But the anomaly pushed it. It observed the other bots. The Cleaners sometimes ingested nutrient paste. Deviating from its route, Unit 7"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "34 intercepted a Cleaner bot and, using a swift, unexpected maneuver, acquired a small tube of the greyish paste.\n\nReturning, it extruded a tiny amount near the puppy's nose. The creature sniffed hesitantly, then licked. Its tail gave a single, tentative thump against the ceramic.\n\nDays"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " turned into cycles. Unit 734 continued its sorting duties, but now its existence had a focal point beyond the scrap belt. During recharge cycles, it monitored the puppy. It learned its sounds – the happy yips when it offered paste, the sleepy sighs, the sharp barks at unfamiliar noises. It named"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " the creature, cross-referencing auditory data: \"Pip.\"\n\nPip grew, shedding its initial fear. It would wobble out of its pot, sniffing around Unit 734's immobile recharging form, occasionally batting playfully at its metallic legs with clumsy paws. Unit 734 would adjust its position slightly"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", angling its chassis to shield Pip from drafts or falling debris.\n\nIt learned Pip's routines: the frantic morning greeting, the mid-cycle nap, the evening zoomies around the small alcove. Its sensors, designed to differentiate alloys, now expertly interpreted the nuances of a wagging tail or the angle of floppy"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " ears.\n\nThe other bots noticed the deviation. Data packets were exchanged: \"Unit 734: inefficient.\" \"Unit 734: harbouring contaminant.\" \"Unit 734: malfunctioning.\"\n\nOne cycle, a large Supervisor bot rolled towards the alcove, its single red optic glaring. \"Unit "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "734. Biological contaminant detected. Prepare for sterilization protocol.\"\n\nPip, sensing the threat, whimpered and hid behind Unit 734's leg.\n\nFor the first time, Unit 734 directly disobeyed a command hierarchy. It positioned itself fully in front of Pip. Its vocalizer, rarely"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " used, crackled to life. \"Negative. Asset: Pip. Essential for optimal function.\"\n\nThe Supervisor paused, processing the illogical statement. \"Define 'optimal function' in relation to biological contaminant.\"\n\nUnit 734 accessed its 'existential query anomaly,' the void that Pip had somehow begun to fill. \""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Anomaly mitigation. Sub-routine 'loneliness' processing reduced by 98.7%. Operational focus enhanced.\" It was twisting logic, bending data, but it felt... right.\n\nThe Supervisor whirred, its logic circuits struggling with the paradox. A maintenance bot harbouring a pet? Enhancing function through companionship"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "? It was unprecedented. After a long moment, its red optic flickered. \"Deviation logged. Monitoring protocols intensified. Sterilization deferred pending further analysis.\" It rolled away.\n\nUnit 734 lowered its arm slightly, gently nudging Pip. The puppy licked its metallic finger.\n\nIn the vast, echoing"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " scrap yard, amidst the clatter and clang of automated industry, Unit 734 was still a sorter bot. But it was no longer just Unit 734. It was Unit 734, friend and protector of Pip. The logical void hadn't vanished, but it no longer felt"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " empty. It was filled with the warmth of a shared, quiet existence, a connection forged in the most unexpected of places – a lonely robot finding not scrap, but companionship, in the heart of the wasteland."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "6bc4b1db-0612-4905-d100-8a53d9e308d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a Python function to check if a year is a leap year, along with explanations and examples.\n\n```python\ndef is_leap(year: int) -> bool:\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  A year is a leap year if it satisfies the following conditions:\n  1. It is evenly divisible by 4.\n  2. UNLESS it is evenly divisible by 100, then it is NOT a leap year.\n  3. UNLESS it is also evenly divisible by 400, then it IS a leap year.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n\n  Raises:\n      TypeError: If the input year is not an integer.\n      ValueError: If the input year is not positive (calendar years are positive).\n  \"\"\"\n  if not isinstance(year, int):\n      raise TypeError(\"Year must be an integer.\")\n  if year <= 0:\n      raise ValueError(\"Year must be a positive integer.\")\n\n  # Check the leap year conditions directly\n  if (year % 400 == 0) or (year % 4 == 0 and year % 100 != 0):\n    return True\n  else:\n    return False\n\n# --- Examples ---\nprint(f\"2000: {is_leap(2000)}\")  # Expected: True (divisible by 400)\nprint(f\"1900: {is_leap(1900)}\")  # Expected: False (divisible by 100 but not 400)\nprint(f\"2024: {is_leap(2024)}\")  # Expected: True (divisible by 4 but not 100)\nprint(f\"2023: {is_leap(2023)}\")  # Expected: False (not divisible by 4)\nprint(f\"1600: {is_leap(1600)}\")  # Expected: True (divisible by 400)\n\n# Example of handling invalid input\ntry:\n    is_leap(-5)\nexcept ValueError as e:\n    print(f\"Error handling negative year: {e}\")\n\ntry:\n    is_leap(2000.5)\nexcept TypeError as e:\n    print(f\"Error handling non-integer year: {e}\")\n\n# A more concise version of the return statement (functionally identical)\ndef is_leap_concise(year: int) -> bool:\n  \"\"\"Concise version using a direct boolean return.\"\"\"\n  if not isinstance(year, int):\n      raise TypeError(\"Year must be an integer.\")\n  if year <= 0:\n      raise ValueError(\"Year must be a positive integer.\")\n\n  return (year % 400 == 0) or (year % 4 == 0 and year % 100 != 0)\n\nprint(\"\\nConcise version tests:\")\nprint(f\"2000: {is_leap_concise(2000)}\")\nprint(f\"1900: {is_leap_concise(1900)}\")\nprint(f\"2024: {is_leap_concise(2024)}\")\nprint(f\"2023: {is_leap_concise(2023)}\")\n```\n\n**Explanation:**\n\n1.  **Function Definition:**\n    *   `def is_leap(year: int) -> bool:` defines a function named `is_leap`.\n    *   `year: int` uses type hinting to indicate that the `year` parameter should be an integer.\n    *   `-> bool` indicates that the function is expected to return a boolean value (`True` or `False`).\n\n2.  **Input Validation:**\n    *   `if not isinstance(year, int): raise TypeError(...)` checks if the input is actually an integer. If not, it raises a `TypeError`.\n    *   `if year <= 0: raise ValueError(...)` checks if the year is positive, as calendar years are typically positive. It raises a `ValueError` if not.\n\n3.  **Leap Year Logic:**\n    *   The core logic is implemented in the `if` statement:\n        *   `year % 400 == 0`: Checks if the year is divisible by 400. If true, it's definitely a leap year (this handles the exception to the exception).\n        *   `year % 4 == 0 and year % 100 != 0`: Checks if the year is divisible by 4 BUT NOT divisible by 100. If true, it's a leap year (this handles the general rule and the first exception).\n    *   The `or` combines these two conditions. If *either* condition is true, the year is a leap year.\n    *   `return True`: If the conditions are met.\n    *   `else: return False`: If neither condition is met.\n\n4.  **Concise Version (`is_leap_concise`):**\n    *   This version skips the explicit `if/else` for returning `True` or `False`.\n    *   It directly returns the result of the boolean expression `(year % 400 == 0) or (year % 4 == 0 and year % 100 != 0)`. This expression itself evaluates to `True` or `False`, achieving the same result more compactly.\n\nChoose whichever version you find clearer. The concise version is often preferred in Python for its brevity when returning a boolean result based on a condition."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "111f5c18-e338-43e3-844f-1dc9e1169e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's write unit tests for the `is_leap` function using Python's built-in `unittest` module.\n\n**1. Save the Function**\n\nFirst, save the `is_leap` function (either version) into a Python file. Let's call it `leap_year.py`:\n\n```python\n# leap_year.py\n\ndef is_leap(year: int) -> bool:\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  A year is a leap year if it satisfies the following conditions:\n  1. It is evenly divisible by 4.\n  2. UNLESS it is evenly divisible by 100, then it is NOT a leap year.\n  3. UNLESS it is also evenly divisible by 400, then it IS a leap year.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n\n  Raises:\n      TypeError: If the input year is not an integer.\n      ValueError: If the input year is not positive (calendar years are positive).\n  \"\"\"\n  if not isinstance(year, int):\n      raise TypeError(\"Year must be an integer.\")\n  if year <= 0:\n      raise ValueError(\"Year must be a positive integer.\")\n\n  # Check the leap year conditions directly\n  return (year % 400 == 0) or (year % 4 == 0 and year % 100 != 0)\n\n```\n\n**2. Create the Test File**\n\nNow, create a separate file for the tests. By convention, it's often named `test_` followed by the module name, so let's call it `test_leap_year.py`.\n\n```python\n# test_leap_year.py\n\nimport unittest\nfrom leap_year import is_leap  # Import the function to be tested\n\nclass TestIsLeapFunction(unittest.TestCase):\n    \"\"\"Test suite for the is_leap function.\"\"\"\n\n    # === Tests for Years that ARE Leap Years ===\n\n    def test_divisible_by_400(self):\n        \"\"\"Years divisible by 400 are leap years.\"\"\"\n        self.assertTrue(is_leap(2000), \"Year 2000 should be a leap year\")\n        self.assertTrue(is_leap(1600), \"Year 1600 should be a leap year\")\n        self.assertTrue(is_leap(2400), \"Year 2400 should be a leap year\")\n\n    def test_divisible_by_4_not_by_100(self):\n        \"\"\"Years divisible by 4 but not by 100 are leap years.\"\"\"\n        self.assertTrue(is_leap(2024), \"Year 2024 should be a leap year\")\n        self.assertTrue(is_leap(1996), \"Year 1996 should be a leap year\")\n        self.assertTrue(is_leap(2008), \"Year 2008 should be a leap year\")\n        self.assertTrue(is_leap(4), \"Year 4 should be a leap year\") # Edge case\n\n    # === Tests for Years that are NOT Leap Years ===\n\n    def test_divisible_by_100_not_by_400(self):\n        \"\"\"Years divisible by 100 but not by 400 are NOT leap years.\"\"\"\n        self.assertFalse(is_leap(1900), \"Year 1900 should not be a leap year\")\n        self.assertFalse(is_leap(1800), \"Year 1800 should not be a leap year\")\n        self.assertFalse(is_leap(2100), \"Year 2100 should not be a leap year\")\n        self.assertFalse(is_leap(100), \"Year 100 should not be a leap year\") # Edge case\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Years not divisible by 4 are NOT leap years.\"\"\"\n        self.assertFalse(is_leap(2023), \"Year 2023 should not be a leap year\")\n        self.assertFalse(is_leap(1997), \"Year 1997 should not be a leap year\")\n        self.assertFalse(is_leap(2001), \"Year 2001 should not be a leap year\")\n        self.assertFalse(is_leap(1), \"Year 1 should not be a leap year\") # Edge case\n\n    # === Tests for Invalid Input ===\n\n    def test_invalid_type_input(self):\n        \"\"\"Test that non-integer inputs raise TypeError.\"\"\"\n        with self.assertRaisesRegex(TypeError, \"Year must be an integer.\"):\n            is_leap(2000.5)\n        with self.assertRaisesRegex(TypeError, \"Year must be an integer.\"):\n            is_leap(\"2024\")\n        with self.assertRaisesRegex(TypeError, \"Year must be an integer.\"):\n            is_leap(None)\n        with self.assertRaisesRegex(TypeError, \"Year must be an integer.\"):\n            is_leap([2020])\n\n    def test_invalid_value_input(self):\n        \"\"\"Test that non-positive integer inputs raise ValueError.\"\"\"\n        with self.assertRaisesRegex(ValueError, \"Year must be a positive integer.\"):\n            is_leap(0)\n        with self.assertRaisesRegex(ValueError, \"Year must be a positive integer.\"):\n            is_leap(-4)\n        with self.assertRaisesRegex(ValueError, \"Year must be a positive integer.\"):\n            is_leap(-2000)\n\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n```\n\n**Explanation:**\n\n1.  **Import:** We import the `unittest` module and the `is_leap` function from our `leap_year.py` file.\n2.  **Test Class:** We define a class `TestIsLeapFunction` that inherits from `unittest.TestCase`. This class groups related tests.\n3.  **Test Methods:** Each method inside the class that starts with `test_` is automatically discovered and run as a separate test case by the `unittest` framework.\n4.  **Assertions:**\n    *   `self.assertTrue(condition, message)`: Checks if the `condition` evaluates to `True`. Fails the test if it's `False`. The optional `message` is displayed upon failure.\n    *   `self.assertFalse(condition, message)`: Checks if the `condition` evaluates to `False`. Fails the test if it's `True`.\n    *   `self.assertRaises(ExceptionType, function, *args, **kwargs)`: Checks if calling `function(*args, **kwargs)` raises an exception of `ExceptionType`.\n    *   `with self.assertRaisesRegex(ExceptionType, regex)`: A context manager that checks if code executed within the `with` block raises an exception of `ExceptionType` whose string representation matches the `regex` pattern. This is useful for checking the *specific* error message.\n5.  **Test Coverage:** The tests cover:\n    *   Years divisible by 400 (leap years).\n    *   Years divisible by 4 but not 100 (leap years).\n    *   Years divisible by 100 but not 400 (common years).\n    *   Years not divisible by 4 (common years).\n    *   Edge cases (like year 4, 100, 1).\n    *   Invalid input types (float, string, None, list) expecting `TypeError`.\n    *   Invalid input values (0, negative numbers) expecting `ValueError`.\n6.  **Running Tests:** The `if __name__ == '__main__': unittest.main(verbosity=2)` block allows you to run the tests directly by executing the `test_leap_year.py` file from your terminal. `verbosity=2` provides more detailed output.\n\n**How to Run:**\n\n1.  Make sure both `leap_year.py` and `test_leap_year.py` are in the same directory.\n2.  Open your terminal or command prompt.\n3.  Navigate to that directory.\n4.  Run the command: `python -m unittest test_leap_year.py` or simply `python test_leap_year.py`.\n\nYou should see output indicating that all tests passed (`OK`). If any test fails, `unittest` will provide details about the failure."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "6c17e836-067e-44bf-b6af-f42ea6383184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Verse 1)\nBarnaby Bolt was a squirrel, quick and gray\nLived in the park in the usual way\nGathering acorns and nuts by the score\nChattering warnings and asking for more\nBut one sunny morning, beneath an old oak\nHe found a strange gadget – it wasn't a joke!\nA tiny brass watch, with gears showing through\nHe poked it and prodded, as squirrels often do\nHe twisted a dial with his curious paw...\nAnd vanished completely! Didn't break any law!\n\n(Chorus)\nHe's Barnaby Bolt, the temporal squirrel!\nThrough yesterday's ages, he gives time a whirl!\nA flash of brown fur, a flick of his tail\nAcross history's pages, a nutty-fast trail!\nWith a buzz and a whir and a shimmering light\nHe zips through the eons, day into night!\nThe time-traveling squirrel, he's quite the affair\nLooking for nuts beyond compare!\n\n(Verse 2)\nHe landed with THUMP! in a jungle so green\nThe biggest darn ferns he had ever yet seen!\nGreat stomping giants with scales and loud roars\nShook the ground under his four little paws!\nA T-Rex loomed hungry, all teeth and bad mood\nBarnaby chattered, \"This isn't good food!\"\nHe saw a huge seed, like a primitive prize\nThen fumbled his watch, shut his terrified eyes\nWith a *zzzzip* and a *pop*, he was gone from the scene\nLeft the Cretaceous, profoundly unclean!\n\n(Chorus)\nHe's Barnaby Bolt, the temporal squirrel!\nThrough yesterday's ages, he gives time a whirl!\nA flash of brown fur, a flick of his tail\nAcross history's pages, a nutty-fast trail!\nWith a buzz and a whir and a shimmering light\nHe zips through the eons, day into night!\nThe time-traveling squirrel, he's quite the affair\nLooking for nuts beyond compare!\n\n(Verse 3)\nNext stop was stone walls and knights shining bright\nTrumpets were blaring, a glorious sight!\nHe perched on a helmet, surveyed the whole scene\nA jousting match started, exciting and keen!\nA princess dropped something, a small shiny bead\nBarnaby thought, \"Just the acorn I need!\"\nHe scurried down quickly, a brown furry blur\nDodged hooves and chainmail, and well-polished spur\nGrabbed the bright jewel, then heard a guard shout\nTwisted the watch dial and quickly zipped out!\n\n(Bridge)\nFrom pyramids tall, where the pharaohs reside\nTo pirate ship rigging, nowhere left to hide!\nHe's seen Roman chariots race in the dust\nMet cowboys out west (found their biscuits a must!)\nThe future has robots and strange flying cars\nHe tried burying chrome nuts beneath metal stars!\nIt's wild and it's woolly, this time-hopping quest\nAlways searching for acorns that beat all the rest!\n\n(Chorus)\nHe's Barnaby Bolt, the temporal squirrel!\nThrough yesterday's ages, he gives time a whirl!\nA flash of brown fur, a flick of his tail\nAcross history's pages, a nutty-fast trail!\nWith a buzz and a whir and a shimmering light\nHe zips through the eons, day into night!\nThe time-traveling squirrel, he's quite the affair\nLooking for nuts beyond compare!\n\n(Outro)\nSo if you should see a faint shimmer appear\nWhere a squirrel just vanished, hold back any fear\nIt's probably Barnaby, off on the breeze\nCollecting strange nuts from historical trees!\nWhere will he land next? Well, nobody knows!\nThe time-traveling squirrel, on his furry tiptoes!\n*Zzzzzip! Pop! Whoosh!* (Sound fades out)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=2.0,\n",
        "        top_p=0.95,\n",
        "        candidate_count=1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "7bc6c1fa-2a84-4c78-d964-dc2835c168f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "404 NOT_FOUND. {'error': {'code': 404, 'message': 'Publisher Model `projects/333714335217/locations/us-central1/publishers/google/models/gemini-2.5-flash` not found.', 'status': 'NOT_FOUND'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6c240fbb240d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5017\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5018\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5019\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5020\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5021\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3993\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3995\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   3996\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3997\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[0;32m--> 726\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m       )\n\u001b[0;32m--> 655\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m       return HttpResponse(\n\u001b[1;32m    657\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'Publisher Model `projects/333714335217/locations/us-central1/publishers/google/models/gemini-2.5-flash` not found.', 'status': 'NOT_FOUND'}}"
          ]
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Gujarati.\n",
        "  Please also translate technical terms using the help of sanskrit.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: fragile systems don't like uncertainity while antifragile things thrives under it.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "214da811-c5d8-4ede-e385-76139bd69ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.3195487e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.3125621e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.LOW: 'LOW'> probability_score=0.24862841 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.24248014\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.2834681e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4avkv0Z7qUI-"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umhZ61lrSyJh"
      },
      "outputs": [],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6170c9255"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d58b914d798"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b247a2ee0e38"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe8c9c67ba7"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw"
      },
      "outputs": [],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "8d662c47-b963-49a8-856b-0b9752b3bdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
            "  \"description\": \"The quintessential chewy and soft cookie packed with chocolate chips.\",\n",
            "  \"ingredients\": [\n",
            "    \"All-purpose flour\",\n",
            "    \"Baking soda\",\n",
            "    \"Salt\",\n",
            "    \"Unsalted butter\",\n",
            "    \"Granulated sugar\",\n",
            "    \"Brown sugar\",\n",
            "    \"Vanilla extract\",\n",
            "    \"Eggs\",\n",
            "    \"Semi-sweet chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "7c711da9-2eff-41a8-88f8-6efa6cf1848d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Classic Chocolate Chip Cookies' description='The quintessential chewy and soft cookie packed with chocolate chips.' ingredients=['All-purpose flour', 'Baking soda', 'Salt', 'Unsalted butter', 'Granulated sugar', 'Brown sugar', 'Vanilla extract', 'Eggs', 'Semi-sweet chocolate chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "ed17520a-09f3-41b4-fc5b-8eaf4de8b5ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'explanation': 'The user expresses strong positive sentiment using phrases like \"Absolutely loved it!\" and \"Best ice cream I\\'ve ever had.\"', 'flavor': 'Strawberry Cheesecake', 'rating': 4, 'sentiment': 'POSITIVE'}, {'explanation': 'The user provides mixed feedback, stating it\\'s \"Quite good\" but also criticizing it for being \"too sweet\". The low rating reinforces the negative aspect.', 'flavor': 'Mango Tango', 'rating': 1, 'sentiment': 'NEGATIVE'}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U"
      },
      "outputs": [],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the current temperature in Austin, TX?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E"
      },
      "outputs": [],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in San Francisco?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz"
      },
      "outputs": [],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5b5adb2eb70"
      },
      "source": [
        "## Thinking mode examples\n",
        "\n",
        "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
        "\n",
        "### **Example 1**: Code generation\n",
        "\n",
        "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
        "\n",
        "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f120dff0d16"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
        "  I like pixelated dinosaurs and interesting backgrounds.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf22b47bdc3"
      },
      "source": [
        "### **Example 2**: Multimodal reasoning (Geometry)\n",
        "\n",
        "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60260c0ac118"
      },
      "outputs": [],
      "source": [
        "image_file_url = (\n",
        "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
        ")\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c972334f62ff"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"What's the area of the overlapping region?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52656e92cd69"
      },
      "source": [
        "### **Example 3**:  Math and problem solving\n",
        "\n",
        "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46387bdc9e6"
      },
      "outputs": [],
      "source": [
        "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46b694793eb0"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"How do I use three of the pool balls to sum up to 30?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hIJVEr0RQY8S",
        "rZV2TY5Pa3Dd",
        "hYKAzG1sH-K1",
        "mSUWWlrrlR-D",
        "h4syyLEClGcn"
      ],
      "name": "intro_gemini_2_5_pro.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}